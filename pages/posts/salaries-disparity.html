<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=devide-width, initial-scale=1.0">
	<title>Salaries of Professors</title>
	<link rel="stylesheet" href="../../style.css">
</head>
<body>
	<div class="wrapper">
		<header>
			<nav>
				<a class="hhome" href="../../index.html" id="profileLink">
					<img id="pfp" width="50" src="../../resources/profiles/OriginalS.png" alt="pfp">
				</a>
				<span style="font-family: msans; font-weight: normal"><span style="font-weight: 800">Naman Taggar's</span> blog</span>
			</nav>
		</header>
		<main>
			<section>
				<p class="blogdesc">
					<a href="../blog.html">Back</a>
				</p>
			</section>
			<section class="blog__post">
				<h2 style="font-family: msans; font-weight: 800; text-transform: uppercase">Salaries
of Professors</h1>
				<p><i>14 February, 2025</i></p>
				<hr><br><br>
<p>I have worked on the <code>salaries</code> dataset twice — once
analysing and visualising it, and secondly introducing a machine
learning algorithm upon it. The salaries dataset is defined as
follows:—</p>
<blockquote>
<p>The 2008–09 nine-month academic salary for Assistant Professors,
Associate Professors and Professors in a college in the U.S. The data
were collected as part of the on-going effort of the college’s
administration to monitor salary differences between male and female
faculty members.</p>
</blockquote>
<p>During this blog post, we will note the disparity in salary based
upon gender of the faculty members. This is both evident in the
visualisations as well as confirmed by the machine learning model we
further introduce. This project was created using Python 3.12.5
(https://python.org).</p>
<p>We first start Python and import the packages required for this
project. These will include pandas used for data analysis and
manipulation, sklearn for applying machine learning, seaborn &amp;
matplotlib for data visualisation.</p>
<pre><code>import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import preprocessing
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from matplotlib import pyplot as plt
import seaborn as sns</code></pre>
<p>Now, we import and present the dataset, followed by giving a brief
outlook of it.</p>
<pre><code>salaries = pd.read_csv(&quot;salaries.csv&quot;)
salaries.head()</code></pre>
<pre><code>#     row     rank discipline  yrs.since.phd  yrs.service gender  salary
#  0    1     Prof          B             19           18   Male  139750
#  1    2     Prof          B             20           16   Male  173200
#  2    3 AsstProf          B              4            3   Male   79750
#  3    4     Prof          B             45           39   Male  115000
#  4    5     Prof          B             40           41   Male  141500</code></pre>
<p>We need to convert all data-values into numeric. Therefore, the
following changes need to be made: convert discipline column to take two
values (0 for A, 1 for B); convert gender column to take two values (0
for male, 1 for female); and convert rank column to take three values (0
for asst. prof., 1 for assoc. prof., 2 for prof.). Other than that, the
data does not require any other pre-processing.</p>
<pre><code>salaries[&quot;discipline&quot;] = salaries[&quot;discipline&quot;].map({&quot;A&quot;: 0, &quot;B&quot;: 1})
salaries[&quot;gender&quot;] = salaries[&quot;gender&quot;].map({&quot;Male&quot;: 0, &quot;Female&quot;: 1})
salaries[&quot;rank&quot;] = salaries[&quot;rank&quot;].map({&quot;AsstProf&quot;: 0, &quot;AssocProf&quot;: 1, &quot;Prof&quot;: 2})</code></pre>
<p>We give a visual presentation of the dataset now (see below). We
notice a disparity amongst the salaries of males and females. Moreover,
direct relations amongst all columns can be seen in the heatmap.</p>
<figure>
<img src="img/salariesgender.png"
alt="Box plot of salaries by gender." />
<figcaption aria-hidden="true">
Box plot of salaries by gender.
</figcaption>
</figure>
<figure>
<img src="img/salarieshm.png" alt="Heat map of the columns." />
<figcaption aria-hidden="true">Heat map of the columns.</figcaption>
</figure>
<p>Furthermore, we show the scatter plot of salary vs. years of
experience and years since PhD, both by gender.</p>
<figure>
<img src="img/svsyrsexp.png"
alt="Salary vs. years of experience, by gender." />
<figcaption aria-hidden="true">
Salary vs. years of experience, by gender.
</figcaption>
</figure>
<figure>
<img src="img/svsyrsphd.png"
alt="Salary vs. years since PhD, by gender." />
<figcaption aria-hidden="true">
Salary vs. years since PhD, by gender.
</figcaption>
</figure>
<p>In both of these, it can firstly be seen that there are far less
females than males, amongst other observations. Out of 397 members, only
39 are female and rest are males.</p>
<p>Of all female faculty members, 23% earn over 1 Cr. annually, and of
all male faculty members, 35.4% males earn over 1 Cr. annually,</p>
<p>Now, We propose a machine learning model, the independent variables
will be rank, discipline, years since PhD &amp; of service, and salary.
Gender will be predicted.</p>
<pre><code>x = salaries.drop([&quot;row&quot;, &quot;gender&quot;], axis=1)
# the column called row has nothing but serial numbers
y = salaries[&quot;gender&quot;]</code></pre>
<p>Now, we form the training and testing datassets.</p>
<pre><code>x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=11, stratify=y)</code></pre>
<p>Some columns have large values, and to make the computations easier
they need to get scaled down. For this purpose, we use the
<code>preprocessing</code> module and create a scaler in order to scale
the data values.</p>
<pre><code>scaler = preprocessing.StandardScaler().fit(x_train)
x_train_scaled = scaler.transform(x_train)</code></pre>
<p>Finally, we now train the dataset and bring our model to the test. If
we look at the <code>gender</code> column in the dataset, we note that
there are far more males than females:—</p>
<pre><code>salaries[&quot;gender&quot;].value_counts()</code></pre>
<pre><code>#  gender
#  0    358
#  1     39
#  Name: count, dtype: int64</code></pre>
<p>Therefore, it is necessary to use
<code>class_weight="balanced"</code> while setting up the logistic
regression model. This counters the heavy bias towards males category in
the dataset. However, due to this, the accuracy on will get affected. We
dropped down to an accuracy score of around 60%.</p>
<pre><code>model = LogisticRegression(class_weight=&quot;balanced&quot;)
model.fit(x_train_scaled, y_train)
train_acc = model.score(x_train_scaled, y_train)
print(&quot;The Accuracy for Training Set is {}%&quot;.format(train_acc*100))</code></pre>
<pre><code>#  The Accuracy for Training Set is 57.413249211356465%</code></pre>
<p>Similarly, the accuracy for testing set can also be found out.</p>
<pre><code>test_acc = accuracy_score(y_test, y_pred)
print(&quot;The Accuracy for Test Set is {}%&quot;.format(test_acc*100))</code></pre>
<pre><code>#  The Accuracy for Test Set is 63.74999999999999%</code></pre>
<p>The classification report is as follows:</p>
<pre><code>x_test_scaled = scaler.transform(x_test)
y_pred = model.predict(x_test_scaled)
print(classification_report(y_test, y_pred))</code></pre>
<pre><code>#                precision    recall  f1-score   support
#  
#             0       0.94      0.64      0.76        72
#             1       0.16      0.62      0.26         8
#  
#      accuracy                           0.64        80
#     macro avg       0.55      0.63      0.51        80
#  weighted avg       0.86      0.64      0.71        80</code></pre>
<p>We can also introduce imaginary data-points in order to see the
results of our model. For example, in the following <code>dpt</code>,
the first element of the tuple is rank 2 which is Professor, second
element is 1 which is discipline B, third element means 30 years of
experience, fourth element means 25 years of experience, and fifth
element means a salary of $180,000. The model predicts that this
data-point is for gender male.</p>
<pre><code>dpt = (2, 1, 30, 25, 180000)
dpt_scaled = scaler.transform([dpt])
model.predict(dpt_scaled)</code></pre>
<pre><code>array([0], dtype=int64)</code></pre>
<p>However, if we edit the datapoint as follows:</p>
<pre><code>dpt = (2, 1, 30, 25, 65000)
dpt_scaled = scaler.transform([dpt])
model.predict(dpt_scaled)</code></pre>
<pre><code>array([1], dtype=int64)</code></pre>
<p>With only a drastic and negative change in the salary, the result is
female. This verifies the disparity amongst salaries of the professors
in the given dataset.</p>
<p>Originally, the model gave an accuracy of 90%. However, this was due
to the fact that the model was predicting all data-points to be males,
and there are around the same number of males (90%) in the data. With a
balanced class-weight, while the accuracy has decreased, but the model
now gives expected and natural results.</p>
				<hr><br><br>
				<p>
					Thanks for reading. Return to see <a href="../blog.html">all blog posts,</a> or to the <a href="../../index.html">home page.</a>
				</p>
			</section>
		</main>
		<footer>&copy; 2024&mdash;25 <strong>Naman Taggar.</strong> <a href="../copy.html">Read more.</a></footer>
	</div>
</body>
</html>
